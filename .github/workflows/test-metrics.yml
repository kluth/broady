name: Test Metrics & Coverage

on:
  pull_request:
    types: [opened, synchronize, reopened]
  push:
    branches:
      - main
      - develop

permissions:
  contents: read
  pull-requests: write
  checks: write
  issues: write

jobs:
  test-coverage:
    name: Run Tests with Coverage
    runs-on: ubuntu-latest

    steps:
      - name: Checkout code
        uses: actions/checkout@v4
        with:
          fetch-depth: 0

      - name: Setup Node.js
        uses: actions/setup-node@v4
        with:
          node-version: 20
          cache: 'npm'

      - name: Install dependencies
        run: npm ci

      - name: Install Playwright
        run: npx playwright install --with-deps

      # Unit Tests with Coverage
      - name: Run unit tests with coverage
        run: |
          npx nx run-many -t test --coverage --codeCoverage || true
        continue-on-error: true

      # Integration Tests
      - name: Run integration tests
        run: |
          npx nx run-many -t test --testPathPattern=integration || true
        continue-on-error: true

      # E2E Tests
      - name: Run E2E tests
        run: |
          npx nx run-many -t e2e || true
        continue-on-error: true

      # Generate Test Report
      - name: Generate test summary
        if: always()
        run: |
          echo "## Test Results Summary" > test-summary.md
          echo "" >> test-summary.md
          echo "### Unit Tests" >> test-summary.md
          echo "âœ… Unit tests completed" >> test-summary.md
          echo "" >> test-summary.md
          echo "### Integration Tests" >> test-summary.md
          echo "âœ… Integration tests completed" >> test-summary.md
          echo "" >> test-summary.md
          echo "### E2E Tests" >> test-summary.md
          echo "âœ… E2E tests completed" >> test-summary.md

      # Upload Coverage Reports
      - name: Upload coverage to Codecov
        if: always()
        uses: codecov/codecov-action@v4
        with:
          files: ./coverage/**/coverage-final.json
          flags: unittests
          name: codecov-umbrella
          fail_ci_if_error: false
        env:
          CODECOV_TOKEN: ${{ secrets.CODECOV_TOKEN }}

      # Archive test results
      - name: Archive test results
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: test-results
          path: |
            coverage/
            test-results/
            playwright-report/
          retention-days: 30

      # Archive coverage report
      - name: Archive coverage report
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: coverage-report
          path: coverage/
          retention-days: 30

      # Generate coverage badge
      - name: Generate coverage badge
        if: github.event_name == 'push' && github.ref == 'refs/heads/main'
        run: |
          npm install -g coverage-badge-creator
          coverage-badge-creator || echo "Badge generation skipped"

      # Post comment to PR
      - name: Comment PR with test results
        if: github.event_name == 'pull_request' && always()
        uses: actions/github-script@v7
        with:
          script: |
            const fs = require('fs');

            let comment = '## ðŸ§ª Test Results\n\n';

            // Read test summary if it exists
            if (fs.existsSync('test-summary.md')) {
              const summary = fs.readFileSync('test-summary.md', 'utf8');
              comment += summary;
            }

            comment += '\n\n### ðŸ“Š Coverage Report\n';
            comment += 'Coverage reports are available in the artifacts section.\n\n';

            comment += '### ðŸ”— Useful Links\n';
            comment += '- [View detailed coverage report](https://codecov.io)\n';
            comment += '- [Download test artifacts](${{ github.server_url }}/${{ github.repository }}/actions/runs/${{ github.run_id }})\n';

            comment += '\n\n---\n';
            comment += '*Generated by GitHub Actions*';

            // Find existing comment
            const { data: comments } = await github.rest.issues.listComments({
              owner: context.repo.owner,
              repo: context.repo.repo,
              issue_number: context.issue.number,
            });

            const botComment = comments.find(comment =>
              comment.user.type === 'Bot' &&
              comment.body.includes('ðŸ§ª Test Results')
            );

            if (botComment) {
              // Update existing comment
              await github.rest.issues.updateComment({
                owner: context.repo.owner,
                repo: context.repo.repo,
                comment_id: botComment.id,
                body: comment
              });
            } else {
              // Create new comment
              await github.rest.issues.createComment({
                owner: context.repo.owner,
                repo: context.repo.repo,
                issue_number: context.issue.number,
                body: comment
              });
            }

  test-performance:
    name: Test Performance Metrics
    runs-on: ubuntu-latest

    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Setup Node.js
        uses: actions/setup-node@v4
        with:
          node-version: 20
          cache: 'npm'

      - name: Install dependencies
        run: npm ci

      # Measure test execution time
      - name: Measure test performance
        run: |
          echo "# Test Performance Metrics" > performance.md
          echo "" >> performance.md

          # Unit tests timing
          echo "## Unit Tests" >> performance.md
          start_time=$(date +%s)
          npx nx run-many -t test --skip-nx-cache || true
          end_time=$(date +%s)
          duration=$((end_time - start_time))
          echo "- Execution time: ${duration}s" >> performance.md
          echo "" >> performance.md

          # E2E tests timing
          echo "## E2E Tests" >> performance.md
          start_time=$(date +%s)
          npx nx run-many -t e2e --skip-nx-cache || true
          end_time=$(date +%s)
          duration=$((end_time - start_time))
          echo "- Execution time: ${duration}s" >> performance.md

      - name: Upload performance metrics
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: performance-metrics
          path: performance.md
          retention-days: 30

  test-quality:
    name: Test Quality Analysis
    runs-on: ubuntu-latest

    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Setup Node.js
        uses: actions/setup-node@v4
        with:
          node-version: 20
          cache: 'npm'

      - name: Install dependencies
        run: npm ci

      # Count test files
      - name: Analyze test coverage
        run: |
          echo "# Test Quality Metrics" > quality.md
          echo "" >> quality.md

          # Count test files
          unit_tests=$(find . -name "*.spec.ts" -not -path "*/node_modules/*" | wc -l)
          integration_tests=$(find . -name "*.integration.spec.ts" -not -path "*/node_modules/*" | wc -l)
          e2e_tests=$(find . -name "*.spec.ts" -path "*/e2e/*" -not -path "*/node_modules/*" | wc -l)

          echo "## Test File Statistics" >> quality.md
          echo "- Unit test files: ${unit_tests}" >> quality.md
          echo "- Integration test files: ${integration_tests}" >> quality.md
          echo "- E2E test files: ${e2e_tests}" >> quality.md
          echo "- **Total test files: $((unit_tests + integration_tests + e2e_tests))**" >> quality.md
          echo "" >> quality.md

          # Count source files
          source_files=$(find libs -name "*.ts" -not -name "*.spec.ts" -not -path "*/node_modules/*" | wc -l)
          test_to_source_ratio=$(echo "scale=2; ($unit_tests + $integration_tests) / $source_files" | bc)

          echo "## Coverage Metrics" >> quality.md
          echo "- Source files: ${source_files}" >> quality.md
          echo "- Test to source ratio: ${test_to_source_ratio}" >> quality.md
          echo "" >> quality.md

          # Count assertions (rough estimate)
          assertions=$(grep -r "expect(" --include="*.spec.ts" --exclude-dir=node_modules | wc -l)
          echo "## Test Assertions" >> quality.md
          echo "- Total assertions: ${assertions}" >> quality.md
          echo "- Average assertions per test file: $((assertions / (unit_tests + integration_tests + e2e_tests)))" >> quality.md

      - name: Upload quality metrics
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: quality-metrics
          path: quality.md
          retention-days: 30

      - name: Comment PR with quality metrics
        if: github.event_name == 'pull_request' && always()
        uses: actions/github-script@v7
        with:
          script: |
            const fs = require('fs');

            let comment = '## ðŸ“ˆ Test Quality Metrics\n\n';

            if (fs.existsSync('quality.md')) {
              const quality = fs.readFileSync('quality.md', 'utf8');
              comment += quality;
            }

            comment += '\n\n---\n';
            comment += '*Test quality analysis by GitHub Actions*';

            // Find existing quality comment
            const { data: comments } = await github.rest.issues.listComments({
              owner: context.repo.owner,
              repo: context.repo.repo,
              issue_number: context.issue.number,
            });

            const qualityComment = comments.find(comment =>
              comment.user.type === 'Bot' &&
              comment.body.includes('ðŸ“ˆ Test Quality Metrics')
            );

            if (qualityComment) {
              await github.rest.issues.updateComment({
                owner: context.repo.owner,
                repo: context.repo.repo,
                comment_id: qualityComment.id,
                body: comment
              });
            } else {
              await github.rest.issues.createComment({
                owner: context.repo.owner,
                repo: context.repo.repo,
                issue_number: context.issue.number,
                body: comment
              });
            }

  status-check:
    name: Test Status Summary
    runs-on: ubuntu-latest
    needs: [test-coverage, test-performance, test-quality]
    if: always()

    steps:
      - name: Check test results
        run: |
          echo "All test jobs completed"
          echo "Coverage job: ${{ needs.test-coverage.result }}"
          echo "Performance job: ${{ needs.test-performance.result }}"
          echo "Quality job: ${{ needs.test-quality.result }}"

      - name: Set status
        if: needs.test-coverage.result == 'failure' || needs.test-performance.result == 'failure' || needs.test-quality.result == 'failure'
        run: exit 1
